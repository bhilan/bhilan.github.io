<!doctype html>
<html class="no-js" lang="">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>The Valley of AI Trust</title>
    <meta name="description" content="Particularly for safety-critical applications or the automation of tasks that can directly impact quality of life, we must be careful to avoid the valley of AI trust—the dip in overall safety caused by premature adoption of automation.">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    

<link rel="apple-touch-icon" sizes="57x57" href="/static/c2c_icons/apple-touch-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/static/c2c_icons/apple-touch-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/static/c2c_icons/apple-touch-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/static/c2c_icons/apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/static/c2c_icons/apple-touch-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/static/c2c_icons/apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/static/c2c_icons/apple-touch-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/static/c2c_icons/apple-touch-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/static/c2c_icons/apple-touch-icon-180x180.png">
<link rel="icon" type="image/png" href="/static/c2c_icons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/static/c2c_icons/favicon-194x194.png" sizes="194x194">
<link rel="icon" type="image/png" href="/static/c2c_icons/favicon-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/static/c2c_icons/android-chrome-192x192.png" sizes="192x192">
<link rel="icon" type="image/png" href="/static/c2c_icons/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="/static/c2c_icons/manifest.json">
<link rel="mask-icon" href="/static/c2c_icons/safari-pinned-tab.svg" color="#773333">
<link rel="shortcut icon" href="/static/c2c_icons/favicon.ico">
<meta name="msapplication-TileColor" content="#773333">
<meta name="msapplication-TileImage" content="/static/c2c_icons/mstile-144x144.png">
<meta name="msapplication-config" content="/static/c2c_icons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">

    <!-- 3rd March 2022 07:41 -->
    <!-- Fonts -->    
    <link href='https://fonts.googleapis.com/css?family=Unica+One' rel='stylesheet' type='text/css'>
    <script src="https://use.typekit.net/qeu2zlf.js"></script>
    <script>try{Typekit.load({ async: false });}catch(e){}</script>
    <noscript>
      <!-- Fallback fonts (if no JS) -->
      <link href='http://fonts.googleapis.com/css?family=Dosis:400|Anonymous+Pro' rel='stylesheet' type='text/css'>
    </noscript>

    <!-- Styles / Scripts -->
    
    <link rel="stylesheet" href="/static/styles/css/cachestocaches.min.css"/>
    <script src="/static/styles/js/modernizr-2.8.3.min.js"></script>

    

  
  
  
  <meta name="twitter:card"
          content="summary_large_image" />
  <meta prefix="og: http://ogp.me/ns#"
          name="twitter:title"
          property="og:title"
          content="The Valley of AI Trust" />
  <meta name="twitter:site"
          content="@CachesToCaches">
  <meta prefix="og: http://ogp.me/ns#"
          property="og:type"
          content="website" />
  
  
    <meta prefix="og: http://ogp.me/ns#"
            name="twitter:image:src"
            property="og:image"
            content="http://cachestocaches.com/static/logos/c2c_logo_card_render.jpg" />
  
  
  <meta prefix="og: http://ogp.me/ns#"
          name="twitter:description"
          property="og:description"
          content="Particularly for safety-critical applications or the automation of tasks that can directly impact quality of life, we must be careful to avoid the valley of AI trust—the dip in overall safety caused by premature adoption of automation." />
  <meta prefix="og: http://ogp.me/ns#"
          property="og:url"
          content="http://cachestocaches.com/2020/3/valley-of-ai-trust/" />

  </head>

  <body
      itemscope
      itemtype="http://schema.org/WebPage"
      data-spy="scroll"
      data-target="#side-nav">
    
    <nav class="sidebar">
      <div class="content">
        
  <div><a href="/"><img src="/static/logos/c2c_logo_2.png" class="img-responsive center-block" style="width:240px;"/></a></div>
  <br/>
  <div class="center-text narrow-font">
    <span><a href="/">home</a></span>
    <span class="center-dot"></span>
    <span><a href="/archive/">archive</a></span>
    <span class="center-dot"></span>
    <span><a href="/contributors/">about</a></span>
  </div>
  <div class="social-icon-container">
    <a class="social-icon light-border twitter" href="https://twitter.com/intent/follow?screen_name=CachesToCaches"></a>
    <a class="social-icon light-border facebook" href="https://www.facebook.com/CachesToCaches"></a>
    <a class="social-icon light-border googleplus" href="https://www.google.com/+Cachestocaches_c2c" rel="publisher"></a>
    <a class="social-icon light-border github" href="https://github.com/CachesToCaches"></a>
    <a class="social-icon light-border rss-feed" href="/feed"></a>
  </div>

  <img class="bottom-art" src="/static/art/c2c_triangles.svg"></img>
  <div class="center-text floating-description"><a href="http://gjstein.com">designed and maintained by<br/>Gregory J Stein</a></div>

        <br/>
        
  <div class="title"><a href="#">The Valley of AI Trust</a></div>
  <hr/>
  <div id="nav-sidebar">
    <ul class="nav" id="toc"></ul>
  </div>
  <hr/>
  
    <br/>
    <div class="title">
      <div><small>part 10 of</small></div>
      <a href="/series/ai-perspectives/" class="nav-title">AI Perspectives</a>
    </div>
    <hr/>
    <div class="description">Reflections on the progress, promise, and impact of AI.</div>
    <hr/>
  

      </div>
    </nav>
    <nav class="header">
      
  <div class="content">
  <span class="heading-font"><a href="/">Caches to Caches</a> | </span><span><a href="/">home</a></span><span class="center-dot"></span><span><a href="/archive/">archive</a></span><span class="center-dot"></span><span><a href="/contributors/">about</a></span>
  </div>

    </nav>
    <div class="page"
         itemsope
         itemtype="http://schema.org/Blog">
      <div class="content-top">
        
  <div class="unit-line-height">
  
  <!-- Series Details -->

  <!-- Post Image -->
    <div class="marginnote invisible-sm post-detail">
      <div>
      
        <p><div class="unit-line-height"><small class="heading-font">
          Part 10 of
        </small></div>
        <div><a class="series heading-font" href="/series/ai-perspectives/">AI Perspectives</a></div>
        </p>
      

        <p><div class="date heading-font">Fri 13 Mar 2020</div>
          <a href="/contributors/#gregory-j-stein"><span class="author" itemprop="name">Gregory J Stein</span></a>
          
        </p>

        
          <p><div class="heading-font">Category</div>
            <a class="category" href="/category/machine-learning/">
              Machine Learning
            </a>
          </p>
        

        
          <p><div class="heading-font">Tags</div>
            
              <a class="tag" href="/tag/editorial/">Editorial</a>
            
              <a class="tag" href="/tag/deep-learning/">Deep Learning</a>
            
          </p>
        
      </div>
    </div>

    
      <h1 class="title">
        <div>
          <span itemprop="name">The Valley of AI Trust</span>
        </div>
      </h1>
    

    


    <div class="note visible-sm post-detail">
      <div>

      <div>
        <span class="date heading-font">Fri 13 Mar 2020</span>
          <a href="/contributors/#gregory-j-stein"><span class="author" itemprop="name">Gregory J Stein</span></a>
          
      </div>

        
          <div>
          <span class="heading-font">Category</span>
            <a class="category" href="/category/machine-learning/">
              Machine Learning
            </a>
          </div>
        

        
          <div><span class="heading-font">Tags</span>
            
              <a class="tag" href="/tag/editorial/">Editorial</a>
            
              <a class="tag" href="/tag/deep-learning/">Deep Learning</a>
            
          </div>
        


        
        <div style="height: 10px"></div>
            <div><span class="unit-line-height"><span class="heading-font">
              Part 10 of
            </span></span>
            <span><a class="series heading-font" href="/series/ai-perspectives/">AI Perspectives</a></span>
            </div>
          
      </div>
    </div>


</div>


      </div>
      <div class="content">
        
  
  <div class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">

  <!-- Post Title -->
  
  

  <!-- Post Content -->
  
    <div class="justify word-break" itemprop="articleBody">
      
        <p>As a researcher at the intersection of Robotics and Machine Learning, the most surprising shift over my five years in the field is how quickly people have warmed to the idea of having AI impact their lives. <a href="https://store.google.com/us/category/google_nest">Learning thermostats</a> are becoming increasingly popular (probably good), digital voice assistants pervade our technology (probably less good), and self-driving vehicles populate our roads (about which I have mixed feelings). Along with this rapid adoption, fueled largely by the hype associated with artificial intelligence and recent progress in machine learning, we as a society are opening ourselves up to risks associated with using this technology in circumstances it is yet unprepared to deal with. Particularly for safety-critical applications or the automation of tasks that can directly impact quality of life, we must be careful to avoid what I call the <em>valley of AI trust</em>—the dip in overall safety caused by premature adoption of automation.</p>
<p></p>
<p>In light of the potential risks, the widespread adoption of AI is perhaps surprising at first glance. Even as late as two years ago, researchers and technologists were predicting that we would need massive performance improvements in AI and the capabilities of autonomous systems before humans would become comfortable with them:</p>
<p><div class="quote-wrapper"><note class="marginnote quote-caption invisible-sm"><div class="heading-font">Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua </div><div><a href="https://arxiv.org/pdf/1708.06374.pdf">On a Formal Model of Safe and Scalable Self-driving Cars</a></div>
<div>Mobileye, 2017</div></note><div class=quote><p>[…] the probability of a fatality caused by an accident per one hour of (human) driving is known to be $10^{−6}$. It is reasonable to assume that for society to accept machines to replace humans in the task of driving, the fatality rate should be reduced by three orders of magnitude, namely a probability of $10^{−9}$ per hour <code>[1]</code>.</p>
<p><code>[1]</code> This estimate is inspired from the fatality rate of air bags and from aviation standards. In particular, $10^{−9}$ is the probability that a wing will spontaneously detach from the aircraft in mid air.</p><note class="quote-caption visible-sm"><div class="heading-font">Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua </div><div><a href="https://arxiv.org/pdf/1708.06374.pdf">On a Formal Model of Safe and Scalable Self-driving Cars</a></div>
<div>Mobileye, 2017</div></note></div></div></p>
<p>This three-orders-of-magnitude improvement threshold for acceptance has not been substantiated by reality, made obvious by the relatively widespread use of Tesla's autopilot feature.</p>
<p>So how did we get here? Why are we seeing increasingly widespread acceptance of AI technologies despite the potential for risk?</p>
<p>Corporate financial incentives clearly play a role, since there is money to be made by being the first companies to provide a certain AI service or automating consumer-facing elements, the rise of <a href="https://www.forbes.com/sites/christopherelliott/2018/08/27/chatbots-are-killing-customer-service-heres-why/#33e9bee513c5">digital chatbots for customer service</a> a notable example. But I might argue that deeper effects are at play. Consumers are often <em>choosing</em> to use these technologies, and Amazon and Facebook are pushing quite hard to win over our affection (though why anyone would pay money to add a <a href="https://www.theverge.com/2018/11/8/18072998/facebook-portal-plus-smart-display-messenger-review-price-specs">Facebook-brand camera</a> in their living room is beyond me). The issue is that most consumers lack an understanding of how much they can <em>trust</em> these automated systems to make effective decisions and under what conditions they might fail.</p>
<p>The hype surrounding "AI" these days certainly is tipping the scales in a potentially dangerous direction. Many big tech companies, including Google, Microsoft, and Amazon Web Services, have positioned themselves as <em>AI companies</em> and their marketing preaches the idea the future they envision this technology will bring is very nearly upon us. In many ways, this narrative is supported by the incredible rate of progress in the research community over the last eight years—AI has achieved superhuman performance at tasks like language translation, playing the game of Go, and object detection, and has enabled mind-bending applications ranging from generative art to modeling protein folding. Referencing its potential for transformative impact, Andrew Ng has even been so ambitious as to call AI "the new electricity". The hype surrounding the incredible capabilities of AI is so strong that a nontrivial portion of the conversation surrounding machine learning in the popular media is devoted to the <em>AI Superintelligence</em>. As a researcher, I occasionally get asked <em>How much should we be worried about AI taking over?</em> One thing I always make clear to people: <em>AI is a lot less intelligent than you think.</em></p>
<p>For the forseeable future, society is substantially more likely to <em>overestimate</em> AI than it is to <em>underestimate</em> its capabilities. <span class="sidenote note no-word-break invisible-sm">The popular media also has a problematic tendency to overhype AI technologies as well, a point covered in a fantastic article in The Guardian entitled "<a href="https://www.theguardian.com/technology/2018/jul/25/ai-artificial-intelligence-social-media-bots-wrong">The discourse is unhinged</a>". </span> Progress on the specific applications that have captured public imagination does not mean that these individual systems can be composed into an AI capable of rivaling human performance in general. IBM Watson is a poignant example of a <a href="https://spectrum.ieee.org/biomedical/diagnostics/how-ibm-watson-overpromised-and-underdelivered-on-ai-health-care">system that illustrates this failing</a>. Watson failed to deliver upon its promises after being welcomed into hospitals to discover connections between patient symptoms that medical staff may have missed.<span class="sidenote note no-word-break invisible-sm">In doing some deeper research on Watson I should point out that IBM has <a href="https://www.ibm.com/blogs/watson-health/ai-healthcare-challenges/">put out a statement</a> that asserts they are "100% focused on patient safety." For all the system's shortcomings, I do not doubt that the researchers and engineers value human life. </span> However, The system made a number of dangerous predictions and recommendations that, if trusted implicitly, could have resulted in serious harm to patients.
<p class="note no-word-break visible-sm">The popular media also has a problematic tendency to overhype AI technologies as well, a point covered in a fantastic article in The Guardian entitled "<a href="https://www.theguardian.com/technology/2018/jul/25/ai-artificial-intelligence-social-media-bots-wrong">The discourse is unhinged</a>". </p><p class="note no-word-break visible-sm">In doing some deeper research on Watson I should point out that IBM has <a href="https://www.ibm.com/blogs/watson-health/ai-healthcare-challenges/">put out a statement</a> that asserts they are "100% focused on patient safety." For all the system's shortcomings, I do not doubt that the researchers and engineers value human life. </p></p>
<p>Despite the hype, there remains a lack of clear delineation between tasks that AI is good at and those on which it is catastrophically bad. For example, consider the task of <em>image generation</em> that has received nontrivial popular attention in the last year or so. Between being asked to generate a "random human face" versus being asked to create an image of "A sheep by another sheep standing on the grass with the sky above and a boat in the ocean by a tree behind the sheep.", on which might you expect that system to do a better job? Perhaps generating faces might be harder, because humans are incredibly discerning when identifying "normal looking" faces. Perhaps, the specificity of the <em>sheep</em> example makes <em>it</em> a more challenging task. The results speak for themselves:</p>
<p><div><note class="marginnote img-caption invisible-sm"><p>Images from <a href="https://arxiv.org/pdf/1912.04958.pdf">StyleGAN2</a> and <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Johnson_Image_Generation_From_CVPR_2018_paper.pdf">Image Generation from Scene Graphs</a> illustrating some state-of-the-art results in the space of image generation using generative adversarial networks.</p></note><img src="/media/photologue/photos/cache/gan_quality_face_vs_sheep_display.jpg" class="img-responsive center-block" title="Image Generation Quality: Faces vs. Sheep" style="max-height:422px"/><note class="img-caption visible-sm"><p>Images from <a href="https://arxiv.org/pdf/1912.04958.pdf">StyleGAN2</a> and <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Johnson_Image_Generation_From_CVPR_2018_paper.pdf">Image Generation from Scene Graphs</a> illustrating some state-of-the-art results in the space of image generation using generative adversarial networks.</p></note></div></p>
<p>So why is the <em>seep example perceptually much worse</em>? The volume of query-specific data required to train the system plays a non-trivial role, since the <em>face generation</em> system has access to many thousands of high-resolution faces, yet the <em>scene generation</em> system is provided with a much more diverse set of images and accompanying natural language descriptions; if even <em>any</em> of those images contained a field with precisely two sheep in it, I would be surprised.<span class="sidenote note no-word-break invisible-sm">If you want to better understand the <em>sheep example</em> I highly recommend you read the <a href="https://arxiv.org/pdf/1804.01622.pdf">Image Generation from Scene Graphs</a> paper from which this result is taken. I have done a poor job here of emphasizing how impressive it is that an AI can be made to generate custom images from complex inputs. </span> In light of this change of perspective, these results are actually quite impressive, since the system generated an image that was potentially <em>completely unlike</em> any image it had ever seen before. However, understanding how and why these systems might perform poorly on a particular task often requires a deep, nuanced understanding of the AI system and the data used to train it. For the average consumer, this level of understanding should not be a requirement.
<p class="note no-word-break visible-sm">If you want to better understand the <em>sheep example</em> I highly recommend you read the <a href="https://arxiv.org/pdf/1804.01622.pdf">Image Generation from Scene Graphs</a> paper from which this result is taken. I have done a poor job here of emphasizing how impressive it is that an AI can be made to generate custom images from complex inputs. </p></p>
<p>To be clear, there are spaces in which AI is already purportedly improving safety: Tesla's self-published safety report even <a href="https://www.tesla.com/VehicleSafetyReport">claims significant safety improvements</a> when using its "autopilot", though the reality is <a href="https://www.nytimes.com/2020/02/25/business/tesla-autopilot-ntsb.html">perhaps more nuanced</a>. Regardless, claims from Elon Musk that <em>full autonomy</em> is coming to Tesla's vehicles <a href="https://www.theverge.com/2019/4/22/18510828/tesla-elon-musk-autonomy-day-investor-comments-self-driving-cars-predictions">as early as late 2020</a> is likely a misleading prediction, one that lends false credibility to the idea that quick progress on highway driving or parking lot navigation implies that the same technology will just-as-quickly enable full <em>Level 5</em> autonomy. For example, driving on the highway is relatively easy, though merging onto an off-ramp is a much more challenging problem, because it requires more reliable predictions of the behavior of the other cars on the road and deeper knowledge of social convention.</p>
<p>So where does this supposed boundary exist? On what tasks should we trust AI? It is difficult to give general instruction about where AI may fail because rarely is the answer to these questions obvious. This deficiency is in part because even the research community often does not know where the lines are. There exist many open questions about the potential capacity for AI to revolutionize our everyday experience, and bold claims of its transformative power are difficult to refute when clear answers do not exist. Additionally, AI often fails in ways that humans do not expect, because its internal representation of the problem or of the data it uses for training is quite different from ours.</p>
<p>In light of these difficulties, our attitude towards new AI systems should be straightforward: <strong>be skeptical</strong>. We are on the cusp of a future in which AI will augment human capacity. Yet, for now, we need to ensure that these technologies are advertised in a way that makes clear what they can and cannot do and what it looks like when they fail. <strong>Trust should be earned</strong>, and must be re-earned when the scope of automation increases.</p>
<p>As always, I welcome discussion in the comments below or on <a href="https://news.ycombinator.com/item?id=22569983">Hacker News</a>.</p>
<h2>References</h2>
<ul>
<li id="shalev2017formal">Shai Shalev-Shwartz, Shaked Shammah & Amnon Shashua, On a Formal Model of Safe and Scalable Self-driving Cars, <i>arXiv preprint arXiv:1708.06374</i>, 2017.</li>
<li id="johnson2018scenegraph">Justin Johnson, Agrim Gupta & Li Fei-Fei, Image Generation from Scene Graphs, <i>in: Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2018.</li>
<li id="karras2019stylegan2">Tero Karras et al., Analyzing and Improving the Image Quality of StyleGAN, <i>arXiv preprint arXiv:1912.04958</i>, 2019.</li>
</ul>
      
    </div>
  

  <!-- Post Footer -->
  

  <!-- Additional Metadata -->
  <meta itemprop="description" content="Particularly for safety-critical applications or the automation of tasks that can directly impact quality of life, we must be careful to avoid the valley of AI trust—the dip in overall safety caused by premature adoption of automation.">

</div>


      </div>
      <div class="content-bottom">
        

  <hr/>
  <p><blockquote>
    Liked this post? Subscribe to our <a class="post-link" href="/feed">RSS feed</a> or add your email to our newsletter:

    <!-- Begin MailChimp Signup Form -->
    <link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
    <div id="mc_embed_signup">
      <form action="https://cachestocaches.us15.list-manage.com/subscribe/post?u=f290745e370ad37f53ed7c7ac&amp;id=b7922d14de" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
        <div id="mc_embed_signup_scroll">
	  
	  <input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
          <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
          <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f290745e370ad37f53ed7c7ac_b7922d14de" tabindex="-1" value=""></div>
          <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
        </div>
      </form>
    </div>

  </blockquote></p>

  <!--End mc_embed_signup-->
  <hr/>


  <!-- All posts in series -->
  
    <div class="post-list">
      <div class="heading-font">all posts in series</div>
      <h3><a href="/series/ai-perspectives/" class="nav-title">AI Perspectives</a></h3>
      <div class="small-line-height">Reflections on the progress, promise, and impact of AI.</div>
      <ul class="small-post-list">
        
          <li ><a class="small-title" href="/2018/7/bias-and-ai/">1 <span class="center-dot"></span> Bias in AI Happens When We Optimize the Wrong Thing</a>
            <div class="small-detail">Bias is a pervasive problem in AI. Only by discouraging machine learning systems from exploiting a certain bias can we expect such a system to avoid doing so.</div></li>
        
          <li ><a class="small-title" href="/2018/9/ai-translation-more-language/">2 <span class="center-dot"></span> For AI, translation is about more than language</a>
            <div class="small-detail">Translation is about expressing the same underlying information in different ways, and modern machine learning is making incredibly rapid progress in this space.</div></li>
        
          <li ><a class="small-title" href="/2018/9/guidelines-practical-ai/">3 <span class="center-dot"></span> Practical Guidelines for Getting Started with Machine Learning</a>
            <div class="small-detail">The potential advantages of AI are many, and using machine learning to accelerate your business may outweigh potential pitfalls. If you are looking to use machine learning tools, here are a few guidelines you should keep in mind.</div></li>
        
          <li ><a class="small-title" href="/2018/12/toward-real-world-alphazero/">4 <span class="center-dot"></span> DeepMind&#x27;s AlphaZero and The Real World</a>
            <div class="small-detail">Using DeepMind&#x27;s AlphaZero AI to solve real problems will require a change in the way computers represent and think about the world. In this post, we discuss how abstract models of the world can be used for better AI decision making and discuss recent work of ours that proposes such a model for the task of navigation.</div></li>
        
          <li ><a class="small-title" href="/2019/1/staggering-amounts-data/">5 <span class="center-dot"></span> Massive Datasets and Generalization in ML</a>
            <div class="small-detail">Big, publically available datasets are great. Yet many practitioners who seek to use models pretrained on this data need to ask themselves how informative the data is likely to be for their purposes. Dataset bias and task specificity are important factors to keep in mind.</div></li>
        
          <li ><a class="small-title" href="/2019/1/proxy-metrics-are-everywhere-machine-lea/">6 <span class="center-dot"></span> Proxy metrics are everywhere in Machine Learning</a>
            <div class="small-detail">Many machine learning systems are optimized using metrics that don&#x27;t perfectly match the stated goals of the system. These so-called &quot;proxy metrics&quot; are incredibly useful, but must be used with caution.</div></li>
        
          <li ><a class="small-title" href="/2019/5/neural-network-structure-and-no-free-lun/">7 <span class="center-dot"></span> No Free Lunch and Neural Network Architecture</a>
            <div class="small-detail">Machine learning must always balance flexibility and prior assumptions about the data. In neural networks, the network architecture codifies these prior assumptions, yet the precise relationship between them is opaque. Deep learning solutions are therefore difficult to build without a lot of trial and error, and neural nets are far from an out-of-the-box solution for most applications.</div></li>
        
          <li ><a class="small-title" href="/2019/8/efficiency-artificial-neural-networks-ve/">8 <span class="center-dot"></span> On the efficiency of Artificial Neural Networks versus the Brain</a>
            <div class="small-detail">Recent ire from the media has focused on the high-power consumption of artificial neural nets (ANNs), yet popular discussion frequently conflates training and testing. Here, I aim to clarify the ways in which conversations involving the relative efficiency of ANNs and the human brain often miss the mark.</div></li>
        
          <li ><a class="small-title" href="/2019/12/my-state-of-the-field/">9 <span class="center-dot"></span> Machine Learning &amp; Robotics: My (biased) 2019 State of the Field</a>
            <div class="small-detail">My thoughts on the past year of progress in Robotics and Machine Learning.</div></li>
        
          <li class="selected"><a class="small-title" href="/2020/3/valley-of-ai-trust/">10 <span class="center-dot"></span> The Valley of AI Trust</a>
            <div class="small-detail">Particularly for safety-critical applications or the automation of tasks that can directly impact quality of life, we must be careful to avoid the valley of AI trust—the dip in overall safety caused by premature adoption of automation.</div></li>
        
      </ul>
    </div>
    <hr/>
  

  <div>
  <div class="social-icon-container visible-xs">
    <a class="social-icon hacker-news"
           href="https://news.ycombinator.com/item?id=22569983"></a><a class="social-icon twitter"
         href="https://twitter.com/intent/tweet?url=http%3A%2F%2Fcachestocaches.com%2F2020%2F3%2Fvalley-of-ai-trust%2F&text=Great%20post%20at"></a><a class="social-icon facebook"
         href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fcachestocaches.com%2F2020%2F3%2Fvalley-of-ai-trust%2F"></a><a class="social-icon googleplus"
         href="https://plus.google.com/share?url=http%3A%2F%2Fcachestocaches.com%2F2020%2F3%2Fvalley-of-ai-trust%2F"></a><a class="social-icon linkedin"
         href="http://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Fcachestocaches.com%2F2020%2F3%2Fvalley-of-ai-trust%2F&title=Caches%20To%20Caches&source=http%3A//cachestocaches.com"></a><a class="social-icon rss-feed"
         href="/feed"></a>
  </div>
</div>


  <hr/>

  <script type="text/javascript">
/* * * CONFIGURATION VARIABLES * * */
var disqus_shortname = 'cachestocaches';

/* * * DON'T EDIT BELOW THIS LINE * * */
 function createCallback() {
     var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
     dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
     (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
   };
</script>
<a class="heading-font" id="disqus_thread" style="cursor:pointer;"onclick="createCallback();return false;"><h4 class="constrain-width">+ Show Comments From Disqus</h4></a>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

  

      </div>
    </div>

    <!-- Footer -->
    <div class="footer">
      <div class="content">
      
  <div><a href="/"><img src="/static/logos/c2c_logo_2.png" class="img-responsive center-block" style="width:240px;"/></a></div>
  <br/>
  <div class="center-text narrow-font">
    <span><a href="/">home</a></span>
    <span class="center-dot"></span>
    <span><a href="/archive/">archive</a></span>
    <span class="center-dot"></span>
    <span><a href="/contributors/">about</a></span>
  </div>
  <div class="social-icon-container">
    <a class="social-icon light-border twitter" href="https://twitter.com/intent/follow?screen_name=CachesToCaches"></a>
    <a class="social-icon light-border facebook" href="https://www.facebook.com/CachesToCaches"></a>
    <a class="social-icon light-border googleplus" href="https://www.google.com/+Cachestocaches_c2c" rel="publisher"></a>
    <a class="social-icon light-border github" href="https://github.com/CachesToCaches"></a>
    <a class="social-icon light-border rss-feed" href="/feed"></a>
  </div>

  <br/>
  <div class="center-text"><a href="http://gjstein.com">designed and maintained by<br/>Gregory J Stein</a></div>

      
      
      </div>
    </div>

    <!-- Scripts -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>

    <!-- Sidebar / Table of Contents -->
    <script src="/static/styles/js/jquery.tableofcontents.g.min.js"></script>
    <script src="/static/styles/js/sticky-nav.js"></script>
    <script src="/static/styles/js/bootstrap-scrollspy.min.js"></script>
    <script>
    $(function(){$("#toc").tableOfContents(null,{
      startLevel: 2, depth: 1});
      $('body').scrollspy({
        target: '#nav-sidebar',
        offset: 20});
    });
    </script>
    
    <!-- Code Highlighting : highlight.js -->
    

<!-- <link rel="stylesheet" href="/static/highlight/styles/c2c.css"> -->
<script src="/static/highlight/highlight.pack.js"></script>
<script>hljs.configure({languages:[]});hljs.initHighlightingOnLoad();</script>
<script src="/static/styles/js/anchor-scroll.js"></script>


    <!-- Equation Rendering : mathjax -->
    <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       jax: ["input/TeX","output/CommonHTML"],
       tex2jax: { inlineMath: [["$","$"]] },
       CommonHTML: {
         linebreaks: { automatic: true, width: "container" }
       }
     });
    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML" type="text/javascript"></script>

    
  <script>
  (function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=
          function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;
          e=o.createElement(i);r=o.getElementsByTagName(i)[0];
          e.src='//www.google-analytics.com/analytics.js';
          r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));
  ga('create','UA-60578350-2','auto');ga('send','pageview');
  </script>

  </body>
</html>
