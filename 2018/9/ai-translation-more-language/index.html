<!doctype html>
<html class="no-js" lang="">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>For AI, translation is about more than language</title>
    <meta name="description" content="Translation is about expressing the same underlying information in different ways, and modern machine learning is making incredibly rapid progress in this space.">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    

<link rel="apple-touch-icon" sizes="57x57" href="/static/c2c_icons/apple-touch-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/static/c2c_icons/apple-touch-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/static/c2c_icons/apple-touch-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/static/c2c_icons/apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/static/c2c_icons/apple-touch-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/static/c2c_icons/apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/static/c2c_icons/apple-touch-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/static/c2c_icons/apple-touch-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/static/c2c_icons/apple-touch-icon-180x180.png">
<link rel="icon" type="image/png" href="/static/c2c_icons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/static/c2c_icons/favicon-194x194.png" sizes="194x194">
<link rel="icon" type="image/png" href="/static/c2c_icons/favicon-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/static/c2c_icons/android-chrome-192x192.png" sizes="192x192">
<link rel="icon" type="image/png" href="/static/c2c_icons/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="/static/c2c_icons/manifest.json">
<link rel="mask-icon" href="/static/c2c_icons/safari-pinned-tab.svg" color="#773333">
<link rel="shortcut icon" href="/static/c2c_icons/favicon.ico">
<meta name="msapplication-TileColor" content="#773333">
<meta name="msapplication-TileImage" content="/static/c2c_icons/mstile-144x144.png">
<meta name="msapplication-config" content="/static/c2c_icons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">

    <!-- 3rd March 2022 07:41 -->
    <!-- Fonts -->    
    <link href='https://fonts.googleapis.com/css?family=Unica+One' rel='stylesheet' type='text/css'>
    <script src="https://use.typekit.net/qeu2zlf.js"></script>
    <script>try{Typekit.load({ async: false });}catch(e){}</script>
    <noscript>
      <!-- Fallback fonts (if no JS) -->
      <link href='http://fonts.googleapis.com/css?family=Dosis:400|Anonymous+Pro' rel='stylesheet' type='text/css'>
    </noscript>

    <!-- Styles / Scripts -->
    
    <link rel="stylesheet" href="/static/styles/css/cachestocaches.min.css"/>
    <script src="/static/styles/js/modernizr-2.8.3.min.js"></script>

    

  
  
  
  <meta name="twitter:card"
          content="summary_large_image" />
  <meta prefix="og: http://ogp.me/ns#"
          name="twitter:title"
          property="og:title"
          content="For AI, translation is about more than language" />
  <meta name="twitter:site"
          content="@CachesToCaches">
  <meta prefix="og: http://ogp.me/ns#"
          property="og:type"
          content="website" />
  
  
    <meta prefix="og: http://ogp.me/ns#"
            name="twitter:image:src"
            property="og:image"
            content="http://cachestocaches.com/static/logos/c2c_logo_card_render.jpg" />
  
  
  <meta prefix="og: http://ogp.me/ns#"
          name="twitter:description"
          property="og:description"
          content="Translation is about expressing the same underlying information in different ways, and modern machine learning is making incredibly rapid progress in this space." />
  <meta prefix="og: http://ogp.me/ns#"
          property="og:url"
          content="http://cachestocaches.com/2018/9/ai-translation-more-language/" />

  </head>

  <body
      itemscope
      itemtype="http://schema.org/WebPage"
      data-spy="scroll"
      data-target="#side-nav">
    
    <nav class="sidebar">
      <div class="content">
        
  <div><a href="/"><img src="/static/logos/c2c_logo_2.png" class="img-responsive center-block" style="width:240px;"/></a></div>
  <br/>
  <div class="center-text narrow-font">
    <span><a href="/">home</a></span>
    <span class="center-dot"></span>
    <span><a href="/archive/">archive</a></span>
    <span class="center-dot"></span>
    <span><a href="/contributors/">about</a></span>
  </div>
  <div class="social-icon-container">
    <a class="social-icon light-border twitter" href="https://twitter.com/intent/follow?screen_name=CachesToCaches"></a>
    <a class="social-icon light-border facebook" href="https://www.facebook.com/CachesToCaches"></a>
    <a class="social-icon light-border googleplus" href="https://www.google.com/+Cachestocaches_c2c" rel="publisher"></a>
    <a class="social-icon light-border github" href="https://github.com/CachesToCaches"></a>
    <a class="social-icon light-border rss-feed" href="/feed"></a>
  </div>

  <img class="bottom-art" src="/static/art/c2c_triangles.svg"></img>
  <div class="center-text floating-description"><a href="http://gjstein.com">designed and maintained by<br/>Gregory J Stein</a></div>

        <br/>
        
  <div class="title"><a href="#">For AI, translation is about more than language</a></div>
  <hr/>
  <div id="nav-sidebar">
    <ul class="nav" id="toc"></ul>
  </div>
  <hr/>
  
    <br/>
    <div class="title">
      <div><small>part 2 of</small></div>
      <a href="/series/ai-perspectives/" class="nav-title">AI Perspectives</a>
    </div>
    <hr/>
    <div class="description">Reflections on the progress, promise, and impact of AI.</div>
    <hr/>
  

      </div>
    </nav>
    <nav class="header">
      
  <div class="content">
  <span class="heading-font"><a href="/">Caches to Caches</a> | </span><span><a href="/">home</a></span><span class="center-dot"></span><span><a href="/archive/">archive</a></span><span class="center-dot"></span><span><a href="/contributors/">about</a></span>
  </div>

    </nav>
    <div class="page"
         itemsope
         itemtype="http://schema.org/Blog">
      <div class="content-top">
        
  <div class="unit-line-height">
  
  <!-- Series Details -->

  <!-- Post Image -->
    <div class="marginnote invisible-sm post-detail">
      <div>
      
        <p><div class="unit-line-height"><small class="heading-font">
          Part 2 of
        </small></div>
        <div><a class="series heading-font" href="/series/ai-perspectives/">AI Perspectives</a></div>
        </p>
      

        <p><div class="date heading-font">Sun 2 Sep 2018</div>
          <a href="/contributors/#gregory-j-stein"><span class="author" itemprop="name">Gregory J Stein</span></a>
          
        </p>

        
          <p><div class="heading-font">Category</div>
            <a class="category" href="/category/machine-learning/">
              Machine Learning
            </a>
          </p>
        

        
          <p><div class="heading-font">Tags</div>
            
              <a class="tag" href="/tag/algorithms/">Algorithms</a>
            
              <a class="tag" href="/tag/deep-learning/">Deep Learning</a>
            
          </p>
        
      </div>
    </div>

    
      <h1 class="title">
        <div>
          <span itemprop="name">For AI, translation is about more than language</span>
        </div>
      </h1>
    

    


    <div class="note visible-sm post-detail">
      <div>

      <div>
        <span class="date heading-font">Sun 2 Sep 2018</span>
          <a href="/contributors/#gregory-j-stein"><span class="author" itemprop="name">Gregory J Stein</span></a>
          
      </div>

        
          <div>
          <span class="heading-font">Category</span>
            <a class="category" href="/category/machine-learning/">
              Machine Learning
            </a>
          </div>
        

        
          <div><span class="heading-font">Tags</span>
            
              <a class="tag" href="/tag/algorithms/">Algorithms</a>
            
              <a class="tag" href="/tag/deep-learning/">Deep Learning</a>
            
          </div>
        


        
        <div style="height: 10px"></div>
            <div><span class="unit-line-height"><span class="heading-font">
              Part 2 of
            </span></span>
            <span><a class="series heading-font" href="/series/ai-perspectives/">AI Perspectives</a></span>
            </div>
          
      </div>
    </div>


</div>


      </div>
      <div class="content">
        
  
  <div class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">

  <!-- Post Title -->
  
  

  <!-- Post Content -->
  
    <div class="justify word-break" itemprop="articleBody">
      
        <p>Just over a two weeks ago, NVIDIA showcased <a href="https://github.com/NVIDIA/vid2vid">vid2vid</a>, their new technique for <em>video-to-video translation</em>. Their paper shows off a number of different applications including one particularly striking example in which the researchers automatically convert sketchy outlines of vlog-style videos from YouTube into <em>compellingly realistic</em> videos of people talking to the camera. The results are incredible and really need to be seen to be believed: </p>
<div class="embed-responsive embed-responsive-16by9">
  <iframe class="embed-responsive-item" src="https://youtube.com/embed/GrP_aOSXt5U"></iframe>
</div>

<p>When most people hear the term "translation" they think of translating natural language: e.g. translating text or speech from Mandarin to English<span class="sidenote note no-word-break invisible-sm">Machine learning is, of course, an incredibly powerful tool for language translation. Recently, researchers from Microsoft <a href="https://blogs.microsoft.com/ai/machine-translation-news-test-set-human-parity/">achieved human-level translation performance</a> on translating news articles from Mandarin to English. </span>. Today I want to reinforce the idea that <em>translation</em> can be applied to different types of data beyond language. The <em>vid2vid</em> paper I mentioned above is just the latest and most visually striking example of the transformative power of AI, and modern machine learning is making incredibly rapid progress in this space.
<p class="note no-word-break visible-sm">Machine learning is, of course, an incredibly powerful tool for language translation. Recently, researchers from Microsoft <a href="https://blogs.microsoft.com/ai/machine-translation-news-test-set-human-parity/">achieved human-level translation performance</a> on translating news articles from Mandarin to English. </p></p>
<p>In the remainder of this article, I will cover:</p>
<ul>
<li>A brief definition of "translation" in the context of AI;</li>
<li>An overview of how modern machine learning systems tackle translation;</li>
<li>A list of application domains and some influential research for each.</li>
</ul>
<p></p>
<h2>What is "Translation"?</h2>
<p>At the crux of translation is the idea of <em>underlying meaning</em>. For instance, the two sentences <em>Je suis heureux</em> and <em>I am happy</em> both convey the same information in their respective languages. To say that a translation is a "good translation" is to say that it preserves the meaning of the sentence when it is converted from one language to another.</p>
<blockquote>
More generally, translation is about expressing the same underlying information in different ways.
</blockquote>

<p>Translation is not at all limited to language: translation from one image "language" to another has been an area of active research in computer vision for the last few years. Though many problems in computer vision, like edge-detection, can be thought of as "translation", only recently have modern deep learning techniques enabled image-to-image translation that are capable of relating color, texture, and even style between different groups of images. This more-modern notion of <em>image-to-image translation</em> is best summarized in the Introduction to the fantastic <a href="https://arxiv.org/pdf/1703.10593.pdf">CycleGAN paper</a>:</p>
<p><div class="quote-wrapper"><note class="marginnote quote-caption invisible-sm"><div class="heading-font">Zhu <em>et al</em> </div><div>Excerpt and image taken from the Introduction of <a href="https://arxiv.org/pdf/1703.10593.pdf">CycleGAN</a>, 2017</div></note><div class=quote><p>What did Claude Monet see as he placed his easel by the bank of the Seine near Argenteuil on a lovely spring day in 1873? A color photograph, had it been invented, may have documented a crisp blue sky and a glassy river reflecting it. Monet conveyed his impression of this same scene through wispy brush strokes and a bright palette.</p>
<p>What if Monet had happened upon the little harbor in Cassis on a cool summer evening? A brief stroll through a gallery of Monet paintings makes it possible to imagine how he would have rendered the scene: perhaps in pastel shades, with abrupt dabs of paint, and a somewhat flattened dynamic range.</p>
<p><img src="/media/photologue/photos/cache/monet_display.jpg" class="img-responsive center-block" title="CycleGAN: Monet Translation" style="max-height:300px"/></p>
<p>We can imagine all this despite never having seen a side by side example of a Monet painting next to a photo of the scene he painted. Instead we have knowledge of the set of Monet paintings and of the set of landscape photographs. We can reason about the stylistic differences between these two sets [of images], and thereby imagine what a scene might look like if we were to "translate" it from one set into the other.</p><note class="quote-caption visible-sm"><div class="heading-font">Zhu <em>et al</em> </div><div>Excerpt and image taken from the Introduction of <a href="https://arxiv.org/pdf/1703.10593.pdf">CycleGAN</a>, 2017</div></note></div></div></p>
<p>Of course, few translations are perfect. In language, idioms or experiences unique to a particular culture have meaning that may be difficult to express in other languages. The subtleties of precisely evaluating translation quality is worth a mention, but is something I will largely avoid for the remainder of the article.</p>
<h2>How does AI translation work?</h2>
<p>Most of the translation systems I will discuss in this article have a similar structure, known as an <em>encoder-decoder</em> architecture. Imagine that you, while on vacation, observe a beautiful vista overlooking the ocean, and decide that you would love friend of yours to reproduce the scene in the style of your favorite painter.<span class="sidenote note no-word-break invisible-sm">In most cases, the <em>encoder-decoder</em> architecture is <em>lossy</em>, meaning that some information is lost in the translation process. However, <a href="https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/">recent research suggests</a> that this structure is actually what enables many modern machine learning system to perform as well as they do. </span> However, your camera is broken, so you instead decide to send her a letter describing the scene. The process of writing the letter is the <em>encoding phase</em>, in which you try to come up with a short list of salient details which you think will be sufficient for painting the scene. In so doing, you may leave out some minutiae, like the exact texture of the rocks nearby or the shapes of all the clouds. Upon receiving the letter, your friend will have to <em>decode</em> your message and paint the picture. It is also common to refer to your letter as the <em>embedded representation</em> of the image, the intermediate data passed between the encoder and decoder.
<p class="note no-word-break visible-sm">In most cases, the <em>encoder-decoder</em> architecture is <em>lossy</em>, meaning that some information is lost in the translation process. However, <a href="https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/">recent research suggests</a> that this structure is actually what enables many modern machine learning system to perform as well as they do. </p></p>
<p><div><note class="marginnote img-caption invisible-sm"><p>This figure illustrates the <em>encoder-decoder</em> architecture. For the neural network, everything is learned, and, as the system trains, the <em>embedded representation</em> (or <em>embedding</em>) typically evolves to include high-level details about the structure of the input image. Input/Output photos are from the <a href="https://taesung89.github.io/cyclegan/2017/03/25/style-transfer-train.html">CycleGAN Project Page</a>.</p></note><img src="/media/photologue/photos/cache/encoder-decoder-schematic3x_display.jpg" class="img-responsive center-block" title="Encoder-Decoder Architecture Diagram" style="max-height:478px"/><note class="img-caption visible-sm"><p>This figure illustrates the <em>encoder-decoder</em> architecture. For the neural network, everything is learned, and, as the system trains, the <em>embedded representation</em> (or <em>embedding</em>) typically evolves to include high-level details about the structure of the input image. Input/Output photos are from the <a href="https://taesung89.github.io/cyclegan/2017/03/25/style-transfer-train.html">CycleGAN Project Page</a>.</p></note></div></p>
<p>For a neural network, everything is typically learned, including both the encoder and decoder blocks and even the <em>embedding space</em>. While the illustrative example I describe above relies on you and your friend communicating via language, there is rarely any such constraint in machine learning systems.<span class="sidenote note no-word-break invisible-sm">Among the key enablers of this technology are <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">Generative Adversarial Networks</a>, which are used to train many of the image-based machine learning systems I discuss throughout the article. </span> As the machine learning systems are trained, they typically learn to preserve the macroscopic features of the input, like the location of mountain ranges or the height of the sun in the sky, while throwing out details like color and texture. It is then the responsibility of the <em>decoder</em> to use it's learned experience to produce the output image in the desired style.
<p class="note no-word-break visible-sm">Among the key enablers of this technology are <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">Generative Adversarial Networks</a>, which are used to train many of the image-based machine learning systems I discuss throughout the article. </p></p>
<h2>Some different types of automated translation</h2>
<h3>Image-to-Image Translation and pix2pix</h3>
<p>Perhaps the biggest milestone in "modern" image-to-image translation was the <a href="https://phillipi.github.io/pix2pix/">Image-to-Image Translation with Conditional Adversarial Nets</a> paper, more commonly known as <em>pix2pix</em>, by Isola <em>et al</em>. The paper includes some incredible visuals, such as colorizing black-and-white photos and generating realistic photos of handbags from edges:</p>
<p><div><note class="marginnote img-caption invisible-sm"><p>Taken from the <em>pix2pix</em> paper, this image shows some highlighted results of successfully "translated" images. For each pair of images, the algorithm generates the image on the right from the input image on the left.</p></note><img src="/media/photologue/photos/cache/pix2pix-cover-photo_display.png" class="img-responsive center-block" title="pix2pix Example Images" style="max-height:403px"/><note class="img-caption visible-sm"><p>Taken from the <em>pix2pix</em> paper, this image shows some highlighted results of successfully "translated" images. For each pair of images, the algorithm generates the image on the right from the input image on the left.</p></note></div></p>
<p>To someone without experience in the field, the translated images are almost so compelling as to be unremarkable, yet it's hard to overstate how exciting these results were when the paper first came out. These results were a <em>big deal</em> at the time, and has spawned significant research since its inception: the <em>pix2pix</em> paper has over 1,000 citations as of the time of this writing, and nearly every paper I discuss in this post cites it as foundational research.<span class="sidenote note no-word-break invisible-sm">I would recommend taking a look at the <em>edge2cats demo</em>. Some of the results are hilarious: <img src="/media/photologue/photos/cache/edges-to-cats-demo_display.png" class="img-responsive center-block" title="edge2cats Demo" style="max-height:347px"/> </span> <em>pix2pix</em> has also inspired a number of interactive online demos, including <a href="https://affinelayer.com/pixsrv/">edge2cats</a>, in which the algorithm will try its best to produce photos of cats from user-drawn sketches. Since the release of <em>pix2pix</em>, other papers have improved the quality and size of the converted images, including the aptly-named <a href="https://github.com/NVIDIA/pix2pixHD"><em>pix2pixHD</em> from NVIDIA</a>.
<p class="note no-word-break visible-sm">I would recommend taking a look at the <em>edge2cats demo</em>. Some of the results are hilarious: <img src="/media/photologue/photos/cache/edges-to-cats-demo_display.png" class="img-responsive center-block" title="edge2cats Demo" style="max-height:347px"/> </p></p>
<h3>Unsupervised Translation and CycleGAN</h3>
<p>As exciting as <em>pix2pix</em> is, it still requires "image pairs" to train the algorithm: i.e. if you want to convert daytime photos to nighttime photos, you will need hundreds or thousands of corresponding daytime and nighttime photos from the same vantage point, which is why that dataset from the <em>pix2pix</em> paper was drawn from timelapse photos of nature-facing cameras. The <a href="https://github.com/junyanz/CycleGAN">CycleGAN paper</a>, which I quote above, introduces a technique to overcome this limitation.<span class="sidenote note no-word-break invisible-sm">There are indeed some technical details I am leaving out for brevity and accessibility, but I highly encourage interested readers to look at the <a href="https://arxiv.org/pdf/1703.10593.pdf">CycleGAN paper</a>, which is quite well-written. </span> Instead of requiring image pairs, the <em>CycleGAN</em> technique only needs two large batches of images: one from each of the target <em>distributions</em>, or <em>languages</em>, of interest.
<p class="note no-word-break visible-sm">There are indeed some technical details I am leaving out for brevity and accessibility, but I highly encourage interested readers to look at the <a href="https://arxiv.org/pdf/1703.10593.pdf">CycleGAN paper</a>, which is quite well-written. </p></p>
<p>The <em>CycleGAN</em> algorithm is designed to recognize common features of each set of images, like how snow is more common in Winter photos and lush grasses are common in Summer photos. As it learns these common features, the algorithm learns to introduce these characteristics during the translation process in an effort to produce realistic images. One of the most striking examples from the CycleGAN paper was <em>horse-to-zebra</em> translation. By providing the algorithm with a few hundred pictures of horses and a few hundred pictures of zebras, the algorithm learns to translate between the two sets and to add stripes to horses as part of the transformation.</p>
<p><div><note class="marginnote img-caption invisible-sm"><p>These stunning images are from the <a href="https://arxiv.org/abs/1703.10593">CycleGAN paper</a>, in which the authors learn a pair of <em>translation networks</em> capable of translating between <em>unpaired</em> sets of images. I previously featured CycleGAN as <a href="/2017/12/favorite-deep-learning-2017/">one of my favorite papers of 2017</a>.</p></note><img src="/media/photologue/photos/cache/bdl17_cyclegan_example_images_display-2_display.jpg" class="img-responsive center-block" title="CycleGAN: some sample results" style="max-height:316px"/><note class="img-caption visible-sm"><p>These stunning images are from the <a href="https://arxiv.org/abs/1703.10593">CycleGAN paper</a>, in which the authors learn a pair of <em>translation networks</em> capable of translating between <em>unpaired</em> sets of images. I previously featured CycleGAN as <a href="/2017/12/favorite-deep-learning-2017/">one of my favorite papers of 2017</a>.</p></note></div></p>
<p>So why is this exciting? Because pairs images can be difficult or even impossible to obtain.<span class="sidenote note no-word-break invisible-sm">In a <a href="https://arxiv.org/pdf/1710.04280.pdf">paper of mine</a> from last year, I use CycleGAN to make simulated images more realistic and then use the resulting image data to train a robot. <img src="/media/photologue/photos/cache/cyclegan_conversion_results_display_display.jpg" class="img-responsive center-block" title="GeneSIS-RT Sample Conversion Results" style="max-height:520px"/> </span> For example, I would be hard-pressed to find a photo of San Francisco in which there were a foot of snow on the ground, however, I can imagine what such a photo might look like because I have an understanding of what cities look like when it snows. CycleGAN, and <a href="https://research.nvidia.com/publication/2017-12_Unsupervised-Image-to-Image-Translation">some similar work from NVIDIA</a>, has enabled research in other domains as well. For example, <a href="https://arxiv.org/pdf/1803.03341.pdf">researchers from Oxford</a> augmented the CycleGAN algorithm to enable better <em>place recognition</em> across times of day, allowing them to more accurately determine the location of their self-driving car at night with only daytime images to compare against.
<p class="note no-word-break visible-sm">In a <a href="https://arxiv.org/pdf/1710.04280.pdf">paper of mine</a> from last year, I use CycleGAN to make simulated images more realistic and then use the resulting image data to train a robot. <img src="/media/photologue/photos/cache/cyclegan_conversion_results_display_display.jpg" class="img-responsive center-block" title="GeneSIS-RT Sample Conversion Results" style="max-height:520px"/> </p></p>
<p>As you may have guessed, unsupervised and "semi-supervised" translation techniques have also shown promise for text data. Researchers at Google are doing some incredible work on what they call <a href="https://ai.googleblog.com/2016/11/zero-shot-translation-with-googles.html">zero-shot Translation</a>. In short, the researchers train a machine learning system capable of translating between many different languages and find that the system eventually learns to translate between languages for which <em>no direct training data was ever provided to the system</em>.<span class="sidenote note no-word-break invisible-sm">The completely unsupervised approach to language translation has just begun to yield some interesting results, as some researchers from Facebook AI just published some remarkable performance numbers on "low resource" languages in <a href="https://arxiv.org/abs/1804.07755">Phrase-Based &amp; Neural Unsupervised Machine Translation</a>. </span> In the example they gave, they trained the system to translate English⇄Korean and English⇄Japanese; the resulting trained system is then capable of reaching nearly state-of-the-art performance for Korean⇄Japanese.
<p class="note no-word-break visible-sm">The completely unsupervised approach to language translation has just begun to yield some interesting results, as some researchers from Facebook AI just published some remarkable performance numbers on "low resource" languages in <a href="https://arxiv.org/abs/1804.07755">Phrase-Based &amp; Neural Unsupervised Machine Translation</a>. </p></p>
<p>To those who speak three or more languages, this result may seem almost trivial. However, the idea that the algorithm is capable of learning a representation of human communication that is <em>language agnostic</em> is an incredible accomplishment for the machine learning community. With this advance, Google Translate now includes more language pairs, since this work has enabled translation to and from "low resource" languages, for which bilingual training data is sparse.</p>
<h3>Video-to-Video Translation and vid2vid</h3>
<p>By now, you probably have a good idea what this section will be about: the recent <a href="https://github.com/NVIDIA/vid2vid">vid2vid paper from NVIDIA</a> enables high-quality translation between <em>videos</em>. The accomplishment here is as much an engineering one as it is an advancement in the state-of-the-art. The key insight in this paper is in the way the learning algorithms are trained.<span class="sidenote note no-word-break invisible-sm">The training procedure for the vid2vid paper is very clearly inspired by other work from NVIDIA, including <a href="https://github.com/NVIDIA/pix2pixHD">pix2pixHD</a> and the "<a href="https://arxiv.org/abs/1710.10196">Progressive Growing of GANs</a>" paper. </span> The team at NVIDIA begin by training the algorithm for single-video-frame translation for low-resolution downsampled video frames. As training progresses, they gradually increase the complexity of the problem by slowly increasing the resolution of the converted frames and adding more frames to be converted at once. By asking the algorithm to begin by solving a much simpler version of the problem and layering on difficulty, the training process is much more stable, and results in much higher quality output.
<p class="note no-word-break visible-sm">The training procedure for the vid2vid paper is very clearly inspired by other work from NVIDIA, including <a href="https://github.com/NVIDIA/pix2pixHD">pix2pixHD</a> and the "<a href="https://arxiv.org/abs/1710.10196">Progressive Growing of GANs</a>" paper. </p></p>
<h3>Image Captioning</h3>
<p>Why should we limit ourselves to translating between input output pairs of the same data type? <em>Image Captioning</em> is the process of generating a sentence to describe an input image. The learning problem for captioning an image shares the same encoder-decoder structure I described above. Yet, as you can imagine, the structures of the encoder and decoder blocks are indeed <em>very</em> different from one another, since the format of the input and output data are so different. Here, the encoding block takes an image as an input while the decoder block takes the resulting <em>embedded representation</em> output and decodes it into a sentence. Because of the way the system is designed, every component in the system can be jointly optimized: the encoder, decoder and embedding representations are all updated in parallel during training to best fit the data.</p>
<p>The landmark work in automated image captioning <a href="https://ai.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html">was first presented by Google in 2014</a>:</p>
<p><div><note class="marginnote img-caption invisible-sm"><p>This image, taken from <a href="https://ai.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html">Google's AI Blog</a> shows some results for their image captioning system.</p></note><img src="/media/photologue/photos/cache/google-image-captioning-results_display.png" class="img-responsive center-block" title="Google Image Captioning" style="max-height:480px"/><note class="img-caption visible-sm"><p>This image, taken from <a href="https://ai.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html">Google's AI Blog</a> shows some results for their image captioning system.</p></note></div></p>
<p>Researchers are also interested in the inverse to the image captioning problem, generating an image corresponding to a sentence. However, language is notoriously vague, so live demos of image generation techniques like <a href="https://arxiv.org/abs/1711.10485">AttnGAN</a> have been the source of some humorous failure cases: <a href="http://aiweirdness.com/post/177091486527/this-ai-is-bad-at-drawing-but-will-try-anyways">This AI is bad at drawing but will try anyways</a>.</p>
<h2>Some final thoughts</h2>
<p>This list is by no means complete: there is too much interesting research for me to have included everything. Many types of problems fit into the translation paradigm. The encoder-decoder architecture I have spent much of this article talking about has proven extremely flexible. I am constantly surprised at how clever the machine learning community is at reusing learned encoder blocks for dimensionality reduction: the <em>embedded representation</em> output from the encoder is typically smaller in size, but should contain most of the "important" information, enabling further learning algorithms to be trained on this intermediate representation.</p>
<p>As always, I welcome discussion in the comments below or on <a href="https://news.ycombinator.com/item?id=17899726">Hacker News</a>. Feel free to ask questions, or let me know of some research I might be interested in.</p>
<h2>References</h2>
<ul>
<li>T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, G. Liu, A. Tao, J. Kautz, and B. Catanzaro. Video-to-Video Synthesis. <em><a href="https://arxiv.org/abs/1808.06601">arXiv:1808.06601</a></em>, 2018.</li>
<li>J.-Y. Zhu, T. Park, P. Isola, and A. Efros. Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. <em><a href="https://arxiv.org/pdf/1703.10593.pdf">In ICCV</a></em>, 2017.</li>
<li>P. Isola, J.-Y. Zhu, T. Zhou, and A. Efros. Image-to-Image Translation with Conditional Adversarial Nets. <em><a href="https://phillipi.github.io/pix2pix/">In CVPR</a></em>, 2017.</li>
<li>G. Stein and N. Roy. GeneSIS-RT: Generating Synthetic Images for training Secondary Real-world Tasks. <em><a href="https://arxiv.org/abs/1710.04280">In ICRA</a></em>, 2017.</li>
<li>T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro. High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs. <em><a href="https://arxiv.org/pdf/1711.11585.pdf">arXiv:1711.11585</a></em>, 2017.</li>
<li>M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised Image-to-Image Translation Networks. <em><a href="https://arxiv.org/abs/1703.00848">In NIPS</a></em>, 2017.</li>
<li>M. Johnson, M. Schuster, Q. Le, M. Krikun, Y. Wu, Z. Chen, N. Thorat, F. Viégas, M. Wattenberg, G. Corrado, M. Hughes, and J. Dean. Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. <em><a href="https://arxiv.org/abs/1611.04558">arXiv:1611.04558</a></em>, 2016.</li>
<li>G. Lample, M. Ott, A. Conneau, L. Denoyer, and M.'A. Ranzato. Phrase-Based &amp; Neural Unsupervised Machine Translation. <em><a href="https://arxiv.org/abs/1804.07755">arXiv:1804.07755</a></em>, 2018.</li>
<li>T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive Growing of GANs for Improved Quality, Stability, and Variation. <em><a href="https://arxiv.org/abs/1710.10196">arXiv:1710.10196</a></em>, 2017.</li>
<li>O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and Tell: A Neural Image Caption Generator. <em><a href="https://arxiv.org/abs/1411.4555">arXiv:1411.4555</a></em>, 2014.</li>
<li>T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and X. He. AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks. <em><a href="https://arxiv.org/abs/1711.10485">arXiv:1711.10485</a></em>, 2017.</li>
</ul>
      
    </div>
  

  <!-- Post Footer -->
  

  <!-- Additional Metadata -->
  <meta itemprop="description" content="Translation is about expressing the same underlying information in different ways, and modern machine learning is making incredibly rapid progress in this space.">

</div>


      </div>
      <div class="content-bottom">
        

  <hr/>
  <p><blockquote>
    Liked this post? Subscribe to our <a class="post-link" href="/feed">RSS feed</a> or add your email to our newsletter:

    <!-- Begin MailChimp Signup Form -->
    <link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
    <div id="mc_embed_signup">
      <form action="https://cachestocaches.us15.list-manage.com/subscribe/post?u=f290745e370ad37f53ed7c7ac&amp;id=b7922d14de" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
        <div id="mc_embed_signup_scroll">
	  
	  <input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
          <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
          <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f290745e370ad37f53ed7c7ac_b7922d14de" tabindex="-1" value=""></div>
          <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
        </div>
      </form>
    </div>

  </blockquote></p>

  <!--End mc_embed_signup-->
  <hr/>


  <!-- All posts in series -->
  
    <div class="post-list">
      <div class="heading-font">all posts in series</div>
      <h3><a href="/series/ai-perspectives/" class="nav-title">AI Perspectives</a></h3>
      <div class="small-line-height">Reflections on the progress, promise, and impact of AI.</div>
      <ul class="small-post-list">
        
          <li ><a class="small-title" href="/2018/7/bias-and-ai/">1 <span class="center-dot"></span> Bias in AI Happens When We Optimize the Wrong Thing</a>
            <div class="small-detail">Bias is a pervasive problem in AI. Only by discouraging machine learning systems from exploiting a certain bias can we expect such a system to avoid doing so.</div></li>
        
          <li class="selected"><a class="small-title" href="/2018/9/ai-translation-more-language/">2 <span class="center-dot"></span> For AI, translation is about more than language</a>
            <div class="small-detail">Translation is about expressing the same underlying information in different ways, and modern machine learning is making incredibly rapid progress in this space.</div></li>
        
          <li ><a class="small-title" href="/2018/9/guidelines-practical-ai/">3 <span class="center-dot"></span> Practical Guidelines for Getting Started with Machine Learning</a>
            <div class="small-detail">The potential advantages of AI are many, and using machine learning to accelerate your business may outweigh potential pitfalls. If you are looking to use machine learning tools, here are a few guidelines you should keep in mind.</div></li>
        
          <li ><a class="small-title" href="/2018/12/toward-real-world-alphazero/">4 <span class="center-dot"></span> DeepMind&#x27;s AlphaZero and The Real World</a>
            <div class="small-detail">Using DeepMind&#x27;s AlphaZero AI to solve real problems will require a change in the way computers represent and think about the world. In this post, we discuss how abstract models of the world can be used for better AI decision making and discuss recent work of ours that proposes such a model for the task of navigation.</div></li>
        
          <li ><a class="small-title" href="/2019/1/staggering-amounts-data/">5 <span class="center-dot"></span> Massive Datasets and Generalization in ML</a>
            <div class="small-detail">Big, publically available datasets are great. Yet many practitioners who seek to use models pretrained on this data need to ask themselves how informative the data is likely to be for their purposes. Dataset bias and task specificity are important factors to keep in mind.</div></li>
        
          <li ><a class="small-title" href="/2019/1/proxy-metrics-are-everywhere-machine-lea/">6 <span class="center-dot"></span> Proxy metrics are everywhere in Machine Learning</a>
            <div class="small-detail">Many machine learning systems are optimized using metrics that don&#x27;t perfectly match the stated goals of the system. These so-called &quot;proxy metrics&quot; are incredibly useful, but must be used with caution.</div></li>
        
          <li ><a class="small-title" href="/2019/5/neural-network-structure-and-no-free-lun/">7 <span class="center-dot"></span> No Free Lunch and Neural Network Architecture</a>
            <div class="small-detail">Machine learning must always balance flexibility and prior assumptions about the data. In neural networks, the network architecture codifies these prior assumptions, yet the precise relationship between them is opaque. Deep learning solutions are therefore difficult to build without a lot of trial and error, and neural nets are far from an out-of-the-box solution for most applications.</div></li>
        
          <li ><a class="small-title" href="/2019/8/efficiency-artificial-neural-networks-ve/">8 <span class="center-dot"></span> On the efficiency of Artificial Neural Networks versus the Brain</a>
            <div class="small-detail">Recent ire from the media has focused on the high-power consumption of artificial neural nets (ANNs), yet popular discussion frequently conflates training and testing. Here, I aim to clarify the ways in which conversations involving the relative efficiency of ANNs and the human brain often miss the mark.</div></li>
        
          <li ><a class="small-title" href="/2019/12/my-state-of-the-field/">9 <span class="center-dot"></span> Machine Learning &amp; Robotics: My (biased) 2019 State of the Field</a>
            <div class="small-detail">My thoughts on the past year of progress in Robotics and Machine Learning.</div></li>
        
          <li ><a class="small-title" href="/2020/3/valley-of-ai-trust/">10 <span class="center-dot"></span> The Valley of AI Trust</a>
            <div class="small-detail">Particularly for safety-critical applications or the automation of tasks that can directly impact quality of life, we must be careful to avoid the valley of AI trust—the dip in overall safety caused by premature adoption of automation.</div></li>
        
      </ul>
    </div>
    <hr/>
  

  <div>
  <div class="social-icon-container visible-xs">
    <a class="social-icon hacker-news"
           href="https://news.ycombinator.com/item?id=17899726"></a><a class="social-icon twitter"
         href="https://twitter.com/intent/tweet?url=http%3A%2F%2Fcachestocaches.com%2F2018%2F9%2Fai-translation-more-language%2F&text=Great%20post%20at"></a><a class="social-icon facebook"
         href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fcachestocaches.com%2F2018%2F9%2Fai-translation-more-language%2F"></a><a class="social-icon googleplus"
         href="https://plus.google.com/share?url=http%3A%2F%2Fcachestocaches.com%2F2018%2F9%2Fai-translation-more-language%2F"></a><a class="social-icon linkedin"
         href="http://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Fcachestocaches.com%2F2018%2F9%2Fai-translation-more-language%2F&title=Caches%20To%20Caches&source=http%3A//cachestocaches.com"></a><a class="social-icon rss-feed"
         href="/feed"></a>
  </div>
</div>


  <hr/>

  <script type="text/javascript">
/* * * CONFIGURATION VARIABLES * * */
var disqus_shortname = 'cachestocaches';

/* * * DON'T EDIT BELOW THIS LINE * * */
 function createCallback() {
     var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
     dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
     (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
   };
</script>
<a class="heading-font" id="disqus_thread" style="cursor:pointer;"onclick="createCallback();return false;"><h4 class="constrain-width">+ Show Comments From Disqus</h4></a>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

  

      </div>
    </div>

    <!-- Footer -->
    <div class="footer">
      <div class="content">
      
  <div><a href="/"><img src="/static/logos/c2c_logo_2.png" class="img-responsive center-block" style="width:240px;"/></a></div>
  <br/>
  <div class="center-text narrow-font">
    <span><a href="/">home</a></span>
    <span class="center-dot"></span>
    <span><a href="/archive/">archive</a></span>
    <span class="center-dot"></span>
    <span><a href="/contributors/">about</a></span>
  </div>
  <div class="social-icon-container">
    <a class="social-icon light-border twitter" href="https://twitter.com/intent/follow?screen_name=CachesToCaches"></a>
    <a class="social-icon light-border facebook" href="https://www.facebook.com/CachesToCaches"></a>
    <a class="social-icon light-border googleplus" href="https://www.google.com/+Cachestocaches_c2c" rel="publisher"></a>
    <a class="social-icon light-border github" href="https://github.com/CachesToCaches"></a>
    <a class="social-icon light-border rss-feed" href="/feed"></a>
  </div>

  <br/>
  <div class="center-text"><a href="http://gjstein.com">designed and maintained by<br/>Gregory J Stein</a></div>

      
      
      </div>
    </div>

    <!-- Scripts -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>

    <!-- Sidebar / Table of Contents -->
    <script src="/static/styles/js/jquery.tableofcontents.g.min.js"></script>
    <script src="/static/styles/js/sticky-nav.js"></script>
    <script src="/static/styles/js/bootstrap-scrollspy.min.js"></script>
    <script>
    $(function(){$("#toc").tableOfContents(null,{
      startLevel: 2, depth: 1});
      $('body').scrollspy({
        target: '#nav-sidebar',
        offset: 20});
    });
    </script>
    
    <!-- Code Highlighting : highlight.js -->
    

<!-- <link rel="stylesheet" href="/static/highlight/styles/c2c.css"> -->
<script src="/static/highlight/highlight.pack.js"></script>
<script>hljs.configure({languages:[]});hljs.initHighlightingOnLoad();</script>
<script src="/static/styles/js/anchor-scroll.js"></script>


    <!-- Equation Rendering : mathjax -->
    <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       jax: ["input/TeX","output/CommonHTML"],
       tex2jax: { inlineMath: [["$","$"]] },
       CommonHTML: {
         linebreaks: { automatic: true, width: "container" }
       }
     });
    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML" type="text/javascript"></script>

    
  <script>
  (function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=
          function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;
          e=o.createElement(i);r=o.getElementsByTagName(i)[0];
          e.src='//www.google-analytics.com/analytics.js';
          r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));
  ga('create','UA-60578350-2','auto');ga('send','pageview');
  </script>

  </body>
</html>
